{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6aad9b6",
   "metadata": {},
   "source": [
    "# Gradient and computational time analysis (via the example of graphene)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb8c06",
   "metadata": {},
   "source": [
    "## 1. Computational time analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f864eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "def crawl_time(fname):\n",
    "    sample_time_list = []\n",
    "    optim_time_list = []\n",
    "    get_time = lambda line: float(line.split(', ')[-1].split(' seconds in total')[0])\n",
    "\n",
    "    with open(fname) as file:\n",
    "        for line in file:\n",
    "            if 'Sampling duration' in line:\n",
    "                sample_time_list.append(get_time(line))\n",
    "            elif 'Optimization duration' in line:\n",
    "                optim_time_list.append(get_time(line))\n",
    "\n",
    "    # remove first few iterations which includes warmup time\n",
    "    min_t = min(len(sample_time_list),len(optim_time_list))\n",
    "    if min_t > 60:\n",
    "        sample_time_list = sample_time_list[60:min_t]\n",
    "        optim_time_list = optim_time_list[60:min_t]\n",
    "    else:\n",
    "        sample_time_list = sample_time_list[1:min_t]\n",
    "        optim_time_list = optim_time_list[1:min_t]\n",
    "    total_time_list = list(np.array(sample_time_list) + np.array(optim_time_list))\n",
    "\n",
    "    return sample_time_list, optim_time_list, total_time_list\n",
    "\n",
    "# replace xxx_slurm.out by an actual slurm output file, which logs the time taken for each step of training\n",
    "flist = [\n",
    "    '_log_graphene_OG_test_multi/_xxxxxx_0_slurm.out', \n",
    "    '_log_graphene_DA_test_multi/_xxxxxx_0_slurm.out' \n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"OG\",\n",
    "    \"DA\",\n",
    "]\n",
    "\n",
    "timelist = [crawl_time(f) for f in flist]\n",
    "samplet_list = [t[0] for t in timelist]\n",
    "optimt_list = [t[1] for t in timelist]\n",
    "totalt_list = [t[2] for t in timelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: outputs the total GPU hours used for OG training with 5 gpus and 80000 iterations\n",
    "i = 0               # specify which setup to check\n",
    "num_gpus = 5        # specify the number of GPUs used\n",
    "num_steps = 80000   # specify the number of total iterations\n",
    "\n",
    "pre = 4\n",
    "mean = np.mean(totalt_list[i])\n",
    "ste = np.std(totalt_list[i]) / np.sqrt(len(totalt_list[i]))\n",
    "\n",
    "labels[i], round(mean, pre) * num_gpus * num_steps / 3600, round(ste, pre) * num_gpus * num_steps / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e325f948",
   "metadata": {},
   "source": [
    "# 2. Gradient evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fafadf",
   "metadata": {},
   "source": [
    "Example code for computing the gradients of the network obtained from doing 1 gradient step at different checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d6ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import mpatch_load_cfg\n",
    "import pickle, os, time\n",
    "from absl import logging\n",
    "\n",
    "sim_num = 50  # number of times to simulate the one-gradient step\n",
    "\n",
    "meta_list = [\n",
    "    [\n",
    "        '_log_graphene_OG_test_multi/', \n",
    "        ['010000', '020000', '030000', '040000', '050000', '060000', '070000', '080000'],\n",
    "    ],\n",
    "    [\n",
    "        '_log_graphene_DA_test_multi/', \n",
    "        ['010000', '020000', '030000', '040000', '050000', '060000', '070000', '080000'],\n",
    "    ],\n",
    "]\n",
    "\n",
    "# read environmental variables for distributed setup\n",
    "if 'COORD_IP' in os.environ and 'PORT' in os.environ:\n",
    "    coord_address = str(os.environ['COORD_IP']).strip()+\":\"+str(os.environ['PORT']).strip()\n",
    "else:\n",
    "    coord_address = None\n",
    "\n",
    "num_processes_str = os.environ.get('NUM_JOBS')\n",
    "num_processes = int(num_processes_str) if num_processes_str else 1\n",
    "\n",
    "process_id_str = os.environ.get('SLURM_ARRAY_TASK_ID')\n",
    "process_id = int(process_id_str) if process_id_str else 0\n",
    "\n",
    "job_id_str = os.environ.get('SLURM_ARRAY_JOB_ID')\n",
    "job_id = int(job_id_str) if job_id_str else None\n",
    "\n",
    "timeout_str = os.environ.get('TIMEOUT')\n",
    "timeout = int(timeout_str) if timeout_str else None\n",
    "\n",
    "dist_initialize = False\n",
    "\n",
    "for meta_info in meta_list:\n",
    "    log_dir = meta_info[0]\n",
    "    ckpts = meta_info[1]\n",
    "    cfg = mpatch_load_cfg(\n",
    "        log_dir=log_dir,\n",
    "        mode='train',\n",
    "        libcu_lib_path='/opt/conda/envs/deepsolid/lib/',\n",
    "        resume=True,\n",
    "        coord_address=coord_address,\n",
    "        num_processes=num_processes,\n",
    "        process_id=process_id,\n",
    "        job_id=job_id,\n",
    "        timeout=timeout,\n",
    "        x64=True,\n",
    "        dist_initialize=dist_initialize,\n",
    "    )\n",
    "\n",
    "    import jax\n",
    "    from DeepSolid import constants\n",
    "\n",
    "    seed = int(1e6 * time.time())\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    sharded_key = constants.make_different_rng_key_on_all_devices(key)\n",
    "    sharded_key, subkeys = constants.p_split(sharded_key)\n",
    "\n",
    "    if dist_initialize is False:\n",
    "        from utils.process import process\n",
    "        from DeepSolid.utils.kfac_ferminet_alpha import utils as kfac_utils\n",
    "        dist_initialize = True\n",
    "\n",
    "    for ckpt in ckpts:\n",
    "        method_dict, result_dict = process(cfg, \n",
    "                                           process_id, \n",
    "                                           get_gradient_for_one_step=True, \n",
    "                                           ckpt_restore_filename=f'{log_dir}qmcjax_ckpt_{ckpt}_process0.npz'\n",
    "        )\n",
    "        sharded_key = method_dict['sharded_key']\n",
    "        new_params_list = []\n",
    "\n",
    "        for i in range(sim_num):\n",
    "            logging.info(f'sim {i}')\n",
    "            sharded_key, subkeys = kfac_utils.p_split(sharded_key)\n",
    "            new_data, _ = method_dict['mcmc_step'](method_dict['old_params'], method_dict['old_data'], subkeys)\n",
    "\n",
    "            new_data = method_dict['mask_if_required'](data=new_data, sharded_key=sharded_key)\n",
    "            processed_data, processed_data_with_keys, sharded_key = method_dict['augment_if_required'](data=new_data, sharded_key=sharded_key)\n",
    "            \n",
    "            sharded_key, subkeys = kfac_utils.p_split(sharded_key)\n",
    "            new_params, _, _ = method_dict['optimizer_step'](  \n",
    "                params=method_dict['old_params'],\n",
    "                rng=subkeys,\n",
    "                data_iterator=iter([processed_data_with_keys]) if method_dict['need_key_for_optim'] else iter([processed_data]),\n",
    "            )\n",
    "            new_params_list.append(new_params)\n",
    "\n",
    "        if mcmc_steps is not None:\n",
    "            with open(f\"{log_dir}params_ckpt_{ckpt}_mcmc{mcmc_steps}.pk\", \"wb+\") as f:\n",
    "                pickle.dump(new_params_list, f)\n",
    "        else:\n",
    "            with open(f\"{log_dir}params_ckpt_{ckpt}.pk\", \"wb+\") as f:\n",
    "                pickle.dump(new_params_list, f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c2110",
   "metadata": {},
   "source": [
    "retrieve std estimates of the gradients (same as that of new_params, since all new_params are obtained from the same init param, and the randomness is over sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af746613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "\n",
    "def convert_dict_to_array(input):\n",
    "    if isinstance(input, dict):\n",
    "        return jnp.concatenate([convert_dict_to_array(input[key]) for key in input])\n",
    "    # else, array\n",
    "    try:\n",
    "        return jnp.array(input).flatten()\n",
    "    except TypeError:\n",
    "        return jnp.concatenate([convert_dict_to_array(item) for item in input])\n",
    "\n",
    "meta_list = [\n",
    "    [\n",
    "        '_log_graphene_OG_test_multi/', \n",
    "        ['010000', '020000', '030000', '040000', '050000', '060000', '070000', '080000'],\n",
    "    ],\n",
    "    [\n",
    "        '_log_graphene_DA_test_multi/', \n",
    "        ['010000', '020000', '030000', '040000', '050000', '060000', '070000', '080000'],\n",
    "    ],\n",
    "]\n",
    "\n",
    "params_all = []\n",
    "for meta_info in meta_list:\n",
    "    log_dir = meta_info[0]\n",
    "    ckpts = meta_info[1]\n",
    "    params_ckpts = []\n",
    "    for ckpt in ckpts:\n",
    "        print(f'Reading {log_dir}params_ckpt_{ckpt}.pk...', end='\\r')\n",
    "        with open(f\"{log_dir}params_ckpt_{ckpt}.pk\", \"rb\") as f:\n",
    "            new_params_dicts = pickle.load(f)\n",
    "        new_params_list = jnp.array([convert_dict_to_array(param_dict) for param_dict in new_params_dicts])\n",
    "        \n",
    "        params_mean = jnp.mean(new_params_list, axis=0)\n",
    "        params_std = jnp.std(new_params_list, axis=0, ddof=1)\n",
    "        params_std_std = jnp.std(jnp.sqrt((new_params_list - params_mean)**2), axis=0, ddof=1)\n",
    "\n",
    "        params_ckpts.append([params_mean, params_std, params_std_std])\n",
    "        \n",
    "    params_all.append(params_ckpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a41bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time taken until each checkpoint to GPU hours\n",
    "\n",
    "gpuhrs_multipliers = [\n",
    "    5 / 3600,\n",
    "    5 / 3600,\n",
    "    5 / 3600,\n",
    "    5 / 3600,\n",
    "    5 / 3600,\n",
    "]\n",
    "\n",
    "gpuhrs_mean_list = [ np.mean(ts) * m for ts, m in zip(totalt_list, gpuhrs_multipliers)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c64566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# std plot\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "label_list = [\n",
    "                r'OG, $N=1000$', \n",
    "                r'DA, $N=90, k=12$',\n",
    "]\n",
    "color_list = [\n",
    "                'black',\n",
    "                'tab:blue', \n",
    "] \n",
    "transparent_ratio = 0.2\n",
    "sim_num = 50 # number of times the one-gradient step has been simulated\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(5,3))\n",
    "\n",
    "for params_ckpts, meta_info, label, color, gpuhrs_mean in zip(params_all, meta_list, label_list, color_list, gpuhrs_mean_list):\n",
    "    t = [int(a) * 1. for a in meta_info[1]]\n",
    "    gpuhrs = np.array(t) * gpuhrs_mean\n",
    "\n",
    "    std_vecs = [p[1] for p in params_ckpts]\n",
    "    error_bars = [p[2]/jnp.sqrt(sim_num) for p in params_ckpts]\n",
    "    sum_stds = [jnp.sqrt(jnp.sum(v**2) / len(v)) for v in std_vecs]\n",
    "    sum_err_bars = [jnp.sqrt(jnp.sum(e**2) / len(e)) for e in error_bars]\n",
    "    sum_lower_err = [s - e for s, e in zip(sum_stds, sum_err_bars)]\n",
    "    sum_upper_err = [s + e for s, e in zip(sum_stds, sum_err_bars)]\n",
    "\n",
    "    prune = lambda x: [b for a,b in zip(gpuhrs,x) if int(a) > 30 and int(a) <= 300]\n",
    "    \n",
    "    gpuhrs = prune(gpuhrs)\n",
    "    sum_stds = prune(sum_stds)\n",
    "    sum_lower_err = prune(sum_lower_err)\n",
    "    sum_upper_err = prune(sum_upper_err)\n",
    "\n",
    "    ax.plot(gpuhrs, sum_stds, '-o', label=label, color=color)\n",
    "\n",
    "    fill_color = np.array(colors.to_rgba(color))\n",
    "    fill_color[3] *= transparent_ratio\n",
    "    ax.fill_between(gpuhrs, sum_lower_err, sum_upper_err, facecolor=fill_color)\n",
    "\n",
    "ax.set_xlim([30,285])\n",
    "ax.legend(loc='lower left', fontsize=10,labelspacing=0)\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "# Enable scientific notation for the axes\n",
    "formatter = ScalarFormatter()\n",
    "formatter.set_scientific(True)  # Enable scientific notation\n",
    "formatter.set_powerlimits((-2, 2))  # Control when scientific notation is used (optional)\n",
    "formatter.useMathText = True\n",
    "\n",
    "# Apply formatter to both x and y axes\n",
    "ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.savefig('gradient_stab_std_gpuhrs.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsolid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
